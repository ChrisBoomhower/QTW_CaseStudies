---
title: "Using Statistics to Identify Spam - Case Study Unit 10"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

####Cory Adams, Chris Boomhower, Alexandra Fisher, Alex Frye
####MSDS 7333, November 11, 2017

***NOTE: We are Answering Q.19 from the Data Science in R textbook (Pg. 168): "Consider the other parameters that can be used to control the recursive partitioning process. Read the documentation for them in the rpart.control() documentation. Also, carry out an Internet search for more information on how to tweak the rpart() tuning parameters. Experiment with values for these parameters. Do the trees that result make sense with your understanding of how the parameters are used? Can you improve the prediction using them?"***

## Abstract
In this case study, over 9,000 email messages are examined in order to develop and test spam filters via classification trees and recursive partitioning. Spam filters are ubiquitous today among email providers and are designed to examine content for red flags as indicators of a wanted or unwanted message, respectively known as ham or spam. Spam data used comes from messages previously classified as either ham or spam by SpamAssassin (http://spamassassin.apache.org) specifically for creating and testing these spam filters [1]. R statistical language is leveraged for spam data cleanup and text processing through the use of decision trees-an approach that uses message characteristics to derive sets of features for email classification. For the purposes of this paper, only the classification tree approach is used and recursive partitioning is the method of choice for building decision trees from derived features. To do this, the rpart partitioning algorithm is used, which exposes several parameters and arguments through the rpart.control function, which enables control of each parameter input [2]. However, the rpart.control function does not accept a list of values for each parameter to generate all possible combinations of the argument values. This is a limitation addressed with custom code. After experimenting with values for these parameters and calculating prediction accuracies for each parameter combination, we find that resulting trees make sense with our understanding of how the parameters are used and we are able to imrpove overall prediction compared to default results. To that end, we find the smaller the cp, minsplit, and minbucket parameter values and larger the maxDepth parameters values, the more accurate predictions are produced.

## Introduction
This case study was undertaken in order to explore and shed light on automating classification procedures for filtering unwanted and potentially harmful email messages, referred to hereafter as spam. Spam filters have become ubiquitous among all email providers and are designed to closely examine content for red flags. After analyzing content for red flag characteristics, spam filters then determine if the message is either a wanted/desired message (ham) or an unwanted message (spam). These types of determinations are rooted in statistical analyses of many emails previously classified as either ham or spam.

Spam data used comes from 9,000 messages previously classified by SpamAssassin (http://spamassassin.apache.org) specifically for creating and testing these spam filters. First and foremost, the information contained in the messages must be organized and processed in order to correctly quantify for further analysis. R statistical language is leveraged to conduct text processing for this initial step and two approaches can be utilized-text mining or decision trees. The text mining approach simply tallies words and compares associated frequencies in ham versus spam, while the decision tree approach uses message characteristics to derive variables for email classification. For the purposes of this case study, only the decision tree approach is used.

To read the 9,000 email messages into R, each file is read as its own message. Various parts are then identified within each message as one of the following: (1) the header (sender and subject information); (2) the message body; or (3) an attachment. However, prior to designing data extraction, we investigate email structures to detail and understand the general message format. Then the headers and message bodies can be processed for extraction. Examples of key information gathered include identifying excessive amounts of punctuation and capitalization in the subject line or character count and frequency in the body. The decision tree is then used to derive variables associated with certain characteristics to classify the messages.

## Background
To build a decision tree from derived features, a recursive partitioning method is applied. The way recursive partitioning works is to divide the data into two groups based on the value of a certain variable. An example of this would be splitting the data based on the amount of capital letters, say if that amount was above or below a set percentage. After the initial split, one group of the data is further divided into two groups. Again, the split is performed according to the value of a specified variable of interest. The data groups are then further divided until all messages have been partitioned into subsets that can be classified as ham or spam based on these characteristics. If the resulting sub-groups created from recursive partitioning are drawn as a diagram, a tree shape is formed, hence the name decision tree.

The overall goal here is to not only to transfer key features into quantifiable measures, but to attempt partitioning the message data into buckets that are ultimately as similar as possible, so either all ham or all spam. Thus, in a single subgroup, observations that comprise a leaf located at the base of the tree are assigned the same classification. As long as leaf messages are as similar as possible, then we can say misclassification errors have been reduced.

Now that we have an understanding of the recursive partition method, we can identify features of interest and develop functions that process email messages into variables. Then we can apply the recursive partitioning method to our derived variables to ultimately determine how well our model predicts email messages as ham or spam. We will also consider other parameters for controlling the recursive partitioning process, specifically experimenting with values for rpart() tuning parameters [2]. Experimenting with parameters will then allow us to more accurately assess the resulting classification trees and if prediction is improved.


## Methods
The steps used for this analysis were: 1) Load and Clean the data; 2) rpart.control Parameter Research; 3) rpart.control Parameter Analysis and Prediction; 4) Comparison to Default Results.

*Note that code used includes modified versions of R code function examples found in Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving, Chapter 3, pages 107-164 [1].*

## Results

### Data Acquisition
```{r echo=FALSE, message=FALSE, warning = FALSE}
####Load Libraries
library(ggplot2)
library(plotly)
library(plyr)
library(grid)
library(gridExtra)
library(formattable)
library(spam)
library(RColorBrewer)
library(rpart)
library(dplyr)
library(corrplot)
library(PerformanceAnalytics)
library(rpart.plot)
library(rpart)
library(RColorBrewer)
library(tm)
```

To begin, we utilized methods provided in the Data Science in R textbook to load a "clean" dataset for spam and ham samples and the associated features. As the purpose of this analysis is to research this data post load, we have excluded the text processing and feature engineering from this analysis. The data is loaded external to the notebook in the ```DataExtractAndClean.R``` file allowing us to focus on our analysis of parameter tuning.

```{r include=FALSE, cache=TRUE, echo = FALSE, warning=FALSE}
source('DataExtractAndClean.R', echo = FALSE)
```


### rpart.control Parameter Research

We have spent significant time to process the data and output features for our model development. However, the goal is fit the best model possible with this data. Once you feel your data is "clean" and accurate, performance gains usually come in the form of tuning the parameters passed to the algorithm. This project uses rpart partitioning algorithm, which exposes several parameters/arguments through the rpart.control function. This function enables us to control each parameter input and compare the results. However, the rpart.control function does not accept a list of values for each parameter to generate all possible combinations of the argument values. This is a limitation we must address with custom code.

The code below will create 1200 combinations of these parameter values we can then pass to the control function for evaluation and identification of the optimal paramater value combination. However, 1200 paramater combinations requires significant processing time. For this reason we sampled the 1200 combinations and selected 240 for processing.

This experiment will focus on the following parameters as input to rpart.control:

  * cp (control) - This parameter is to optimize computing time by removing splits that do not increase the R-squared value by the defined cp value at each step. For this project we have chosen the following cp values: 0.00001, 0.0001, 0.005, and 0.01.
  
  * minsplit (control) - rpart builds models that result in tree. This tree must have branches and values leading to an end node result. The minsplit parameter defines the number of observations necessary for a split or branch to be considered. For this project we have chosen to have a 2:1 ratio with minbucket. Keeping the same ratio allows us to normalize the parameter values.

  * minbucket (control) - This parameter determines the lowest possible value for the number of observations in an end node. For this project we have chosen to have a 1:2 ratio with minsplit. Keeping the same ratio allows us to normalize the parameter values.
  
  * maxdepth (control) - This parameter defines how many levels of nodes (splits) the final output tree may contain. If the tree contains too many levels, or too much depth, it will likely output poor results (potential overfitting). 
  
Again, the code below will output a dataframe with 240 parameter combinations sampled from the initial 1200 parameter combinations created using the cp, minsplit, minbucket, and maxdepth parameters. Each of these parameter combinations are now ready to be passed to the rpart.control function (in a loop) to identify the optimal combination. The graphic below shows the first 10 combinations of sampled parameters merged into a single dataframe. This provides a good representation of the parameter combinations we developed and demonstrates the spread of values tested.

Total parameter combinations generated: ````r paramCombos```` <br>
Parameter Combinations Sampled to: ````r paramCombos/5````

```{r}
#cost<-data.frame(cost=c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 5.0))

cp<-data.frame(cp=c(0.00001, 0.0001, 0.005, 0.01))

bucketSplit<-data.frame(minsplit =round(seq(2, 100, length=30),0)+ (round(seq(2, 100, length=30),0)%%2))
bucketSplit$minbucket <- bucketSplit$minsplit/2

maxDepth<-data.frame(maxDepth = round(seq(3, 30, length=10),0))

#parameters<-merge(merge(merge(cost,cp,all=TRUE),bucketSplit,all=TRUE),maxDepth,all=TRUE)
parameters<-merge(merge(cp,bucketSplit,all=TRUE),maxDepth,all=TRUE)

paramCombos <- nrow(parameters)

set.seed(paramCombos)

paramSampleddf<- sample_n(parameters, (paramCombos/5))

formattable(head(paramSampleddf,10))

```


### rpart.control Parameter Analysis and Prediction
Having now defined our 240 unique combinations of cp, minsplit, minbucket, and maxDepth, we proceed with rendering our various partitioning trees to compare performance. Below, we segment each configuration into a list of 240 unique parameter combinations and then use lapply() to apply the rpart() function using each configuration. 
```{r}
paramSampled<-setNames(split(paramSampleddf, seq(nrow(paramSampleddf))), rownames(paramSampleddf))

fits <- lapply(paramSampled, function(x) {
  rpartObj = rpart(isSpam ~ ., data = trainDF,
                   method="class", 
                   #cost = x$cost,
                   control = rpart.control(cp = x$cp, minsplit = x$minsplit, minbucket = x$minbucket, maxdepth = x$maxDepth) )
  
  predict(rpartObj, 
          newdata = testDF[ , names(testDF) != "isSpam"],
          type = "class")
  })

```

Upon implementing the above code chunk, a new list of prediction results (length 240) for the 3116 test messages is rendered. This means we now have 240 sets of prediction results for all 3116 test messages. We use this list to compute accuracies and compare overall classification performance of our trees using each input parameter configuration set.

Prediction accuracies using each parameter combination are calculated next. This is done by processing our list of results and creating a new accuracy column in our previous parameter combination dataframe. The top 10 most accurate combinations are presented in the table that follows.

```{r warning=FALSE}
spam = testDF$isSpam == "T"

accuracy = lapply(fits,function(x){
  xtab <- table(unlist(x), spam)
  #print(xtab)
  sum(diag(xtab))/sum(xtab)

  })

paramSampleddf$Accuracies <- unlist(accuracy)

maxAccuracy <- max(paramSampleddf$Accuracies)
formattable(head(paramSampleddf[order(-paramSampleddf$Accuracies),],10))
```

While this table is informative, the fact that we sampled our 1200 parameter combinations down to 240 makes it difficult to interpret with much granularity. For this reason, we take a novel approach to parameter assessment next. We've rendered the following scatterplot and correlation matrix comprised of rpart()/rpart.control() parameters and final accuracies to better assess performance. By visually treating accuracy as we would a response variable, and cp, minsplit, minbucket, and maxDepth as we would independent variables, we are able to observe correlations between each parameter and model accuracy.

```{r warning=FALSE}
res <- cor(paramSampleddf)

chart.Correlation(paramSampleddf, histogram=TRUE, pch=19)

```

In our above scatterplot and correlation matrix, the distribution of each rpart() or rpart.control() parameter is shown along the diagonal. Reviewing these histogram distributions reveals that a mostly even distribution of parameter values made it into the 240 configuration samples. This suggests we are assessing a wide range of model accuracy outcomes based on parameter selection.

Further supporting this argument, the bottom-left scatterplots depict bivariate relationships among the parameter values. A rolling fit line is also provided in these plots, but like the general bivariate relationships among parameters, we only care about the fit line trend for the bottom row where parameters are compared against accuracy.

In the top-right cells, bivariate correlations are provided (Pearson's R values). Similarly, relationship significance levels are indicated by stars. Three stars represent a p-value less than 0.001; two stars, a p-value between 0.001 and 0.01; one star, a p-value between 0.01 and 0.05; and a period, a p-value between 0.05 and 0.1. Again, these values are only important when comparing parameters against accuracy. So in this case, only the values in the rightmost column are relevant to us.

Based on these matrix descriptions, we can quickly determine that accuracy decreases slightly as cp increases (weak negative correlation of -0.17), accuracy decreases more rapidly as either minsplit or minbucket increases (moderate negative correlation of -0.4 for each), and accuracy improves as maxDepth is increased (moderate positive correlation of 0.51). Reviewing the scatterplots and fit lines, we discern that while cp, minsplit, and minbucket relationships to accuracy are relatively linear, maxDepth is not. In contrast, it portrays a change in trend around a maxDepth value of 14. Prior to this value, accuracy rapidly improves as maxDepth increases, but after this value, improvement to accuracy is virtually non-existant. Therefore, a maxDepth of 14 seems to be the point after which we see diminishing returns when increasing our maxDepth value.

Based on these evaluations, it is clear why a cp value of 0.00001, minsplit of 2, minbucket of 1, and maxDepth of 21 produced the most accurate model. The smallest cp, minsplit, and minbucket values, and larger maxDepth values produce the best results. Granted, our accuracy would have been relatively the same had we had a maxDepth value of 14; again, we only sampled 240 parameter configurations among our original 1200. Based on our discussions in the *rpart.control Parameter Research* section, the new trees generated using these input parameters make sense.

### Comparison to Default Results

Finally, with our best tree parameter combination identified above, we may compare performance against this case study's original tree's accuracy (````r defaultAccuracy*100````%). The original tree was configured with default parameters as depicted in the following code chunk. By implementing our tuned partitioning tree, we are able to gain an accuracy improvement of ````r (maxAccuracy-defaultAccuracy)*100````%. Given the minimal effort required to tune our model with these parameters alone, this is a satisfactory improvement indeed.

```{r}
#code here to run defaults

rpartObj = rpart(isSpam ~ ., data = trainDF, method="class")

defaultResults <- predict(rpartObj, 
        newdata = testDF[ , names(testDF) != "isSpam"],
        type = "class")

xtab <- table(unlist(defaultResults), spam)
defaultAccuracy<-sum(diag(xtab))/sum(xtab)
```


## Discussion and Future Works 
Following intensive text processing and model development, the goal ultimately becomes fitting the best model possible with this spam data and as noted previously, performance gains come from tuning parameters passed through the rpart partitioning algorithm in which the rpart.control function enables us to control each parameter input and compare results. To that end, due to time constraints and processing power, we sampled a total of 1,200 parameter combinations resulting in a total of 240 unique combinations of cp, minsplit, minbucket, and maxDepth parameters. Then we were able to render various partitioning trees to compare performance.

After experimenting with values for these parameters and calculating prediction accuracies for each parameter combination, we found the top 10 most accurate combinations; however, while this table output was informative, it was too difficult to interpret on a granular level. Thus, we implemented a novel approach to parameter assessment by rendering a more intuitive visualization in the form of a scatterplot and correlation matrix comprised of rpart()/rpart.control() parameters and final accuracies for performance assessment. By visually treating accuracy as we would a response variable, and cp, minsplit, minbucket, and maxDepth as we would independent variables, we were able to observe correlations between each parameter and model accuracy. In reviewing histogram distributions here, we found a mostly even distribution of parameter values, which suggests we are assessing a wide range of model accuracy outcomes based on parameter selection. Scatterplots further supported this by depicting bivariate relationships among values.

Looking more closely at scatterplots and fit lines, we determined that prediction accuracy decreases slightly as cp increases, accuracy decreases more rapidly as either minsplit or minbucket increases, and accuracy improves as maxDepth is increased. In addition, we found that while cp, minsplit, and minbucket have fairly linear relationships to accuracy, maxDepth does not as it shows an obvious trend change around a maxDepth value of 14, i.e. the point after which we see diminishing returns when increasing the maxDepth value. Therefore, we find an overall improvement in prediction and the most accurate model produced with inputs of cp value of 0.00001, minsplit of 2, minbucket of 1, and maxDepth of 21. Also, the smallest cp, minsplit, and minbucket values and larger maxDepth values produce the best results. Granted, our accuracy would have been relatively the same had we had a maxDepth value of 14; again, we only sampled 240 parameter configurations among our original 1,200. Based on our discussions in the rpart.control Parameter Research section, the new trees generated using these input parameters make sense. Finally, when comparing our best tree parameter combination with default results, we found an improvement in prediction accuracy by ````r (maxAccuracy-defaultAccuracy)*100````%%.

The overall prediction improvement is not only a successful outcome, but also a direct reflection of the importance and usefulness of testing a variety of parameter inputs to distort the computations within the model. To further our understanding of the effectiveness of these models cross validation would be needed to be assessed through the utilization of the "xval" parameter in the rpart.control() function, when selecting among competing models. Cross validation can ensure that amongst varying combinations of data, the model holds true under each input, allowing for a more robust understanding of model fit and avoiding overfitting the data. Also, further research could be necessary to take a more manual approach to modifications to inputs surrounding our winning model. Due to the "random" nature of our parameter tests, there may be further winnings with additional research. Finally, precision and recall may be of interest as a supplement to our result accuracy metric to ensure we have chosen the best model. As an additional extension of this effort, future works would also include employing a text mining Naïve Bayes approach for a side-by-side comparison of model predictions.

## References
[1] D. Lang and D. Nolan, Data Science in R: A Case Studies Approach to Computation Reasoning and Problem Solving. New York, New York: CRC Press. 
[2] B. Atkinson, B. Ripley, & T. Therneau, Package 'rpart'. Version 4.1-11. Online: 2017-03-12. https://cran.r-project.org/web/packages/rpart/rpart.pdf 
