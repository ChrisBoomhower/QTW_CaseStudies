---
title: "Simulation Study of a Branching Process - Case Study Unit 12"
output: html_notebook
---


####Cory Adams, Chris Boomhower, Alexandra Fisher, Alex Frye
####MSDS 7333, November 26, 2017

***NOTE: We are answering Q.10 from the Data Science in R textbook (Pg. 307): "Carry out a simulation study to see if the re-parameterization suggested in Section 7.9.1 is appropriate. For example, fix kappa to be 1, and run the simulation for various values of lambda. Compare the results to other simulations where kappa is c not equal to 1, but the ratio of lambda/c matches one of the lambda values from the earlier simulation when kappa was 1."***


## Abstract


## Introduction
In this case study, we are studying how programs/jobs running on a CPU, or multiple CPUs in parallel, behave. Multiple CPUs running in parallel, known as parallel computing, enable us to break up programs into smaller portions that can run simultaneously on different CPUs. Specifically, we investigate the distribution of the completion time for the entire system to determine which combinations of job creation and job completion rates leads to processes that either never complete or definitely complete in finite time. 

To do this, we carry out a simulation study for the purpose of seeing if re-parameterization is appropriate. The branching process we investigate encompasses the same characteristics of the Aldous and Krebs process [3]. During the lifetime of a program/job, the number of births a job has is assumed to be from a Poisson distribution with expected count lambda. Also, the life length of each job is assumed to be from an exponential distribution with expected life length of 1/kappa. Lastly, each job can immediately start creating, or birthing, new jobs as soon as the job is generated, but the offspring/children cannot begin running until their parent finishes. The Monte Carlo method is used to simulate the process with different values for each job's expected lifetime (1/kappa) and number of jobs created per unit of time (lambda). 


## Background
John Tsitsiklis, Christos Papadimitriou, and Pierre Humblet studied how the systems of interdependent jobs behave in their study entitled “The Performance of a Precedence-Based Queuing Discipline” published in the Journal of the Association for Computing Machinery. In this paper, Tsitsiklis et. al. focused on finding the distribution of the length of time that a computational process, including all of its subtasks, takes to complete [2]. The overarching goal was to aid developers in designing code that could run efficiently while in parallel or queuing systems that could manage jobs on a compute cluster. To do this, a probability model was proposed for the generation of jobs and their associated interdependencies; however, using this system to answer questions of interest proved too challenging and mathematically difficult. 

David Aldous and William Krebs sought to overcome these issues via the use of a slightly different branching process, named the Aldous and Krebs process [3]. The Aldous and Krebs process is one in which: (1) the first job creates other jobs with time between start of jobs that is both independent and identically distributed, (2) the subsequent jobs generated must all wait until their parent program completes before they can actually run, and (3) each job can produce additional jobs of its own as soon as it is created so that it is not required to wait to start running before it can create another job. Through the study of this process, Aldous and Krebs were able to use mathematics to find analytic solutions to features of this process, just as Tsitsiklis et. al. had done. 

Tsitsiklis et. al. and Aldous and Krebs derived these solutions by studying ways to simplify more complex problems. Another way to do this would be via the use of simulations in place of or in addition to these closed-form analytic solutions because they may be overly cumbersome to solve mathematically. Simulations can also be useful for providing insight when we want to study the behavior of a process, but the assumed behavior is violated. In other words, simulation studies aim to model random processes via the use of a computer to gain deeper insights about associated properties. For the purposes of this study, the Monte Carlo method is used for simulation. 


## Methods
The steps used for this analysis were: 1) ; 2) ; 3) ; 4) ...

*Note that code used includes modified versions of R code function examples found in Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving, Chapter 7, pages 277-306 [1].*

## Results

### Data Acquisition
```{r echo=FALSE, message=FALSE, warning = FALSE}
# Load Libraries
library(scatterplot3d)
```



```{r include=FALSE, cache=TRUE, echo = FALSE, warning=FALSE}
source('DataExtractAndClean.R', echo = FALSE)
```

```{r}
# To load the original simulation results, use readRDS() as follows
orig.mcGrid = readRDS("mcGridOutput.rds")

# Following code was commented from DataExtractAndClean.R but pasted below for modification/use in notebook comparisons
logUQkids = sapply(orig.mcGrid, function(x) 
    log(quantile(x[2, ], probs = 0.75), base = 10))

UQCut = cut(logUQkids, breaks = c(-0.1, 0.5, 2, max(logUQkids)) )
color3 = c("#b3cde3aa", "#8856a7aa", "#810f7caa")
colors = color3[UQCut]

#3Dscatter
sdp = scatterplot3d(x = paramGrid[ , 1], y = paramGrid[ , 2], 
                    z = logUQkids, pch = 15, color = colors,
                    xlab = "Lambda", ylab = "Kappa",
                    zlab = "Upper Quartile Offspring",
                    angle = 120, type="h")

legend("left", inset = .08, bty = "n", cex = 0.8,
       legend = c("[0, 0.5)", "[0.5, 2)", "[2, 5)"), 
       fill = color3)

#BA_ImageMapAlive
oldPar  = par(mar = c(4.1, 4.1, 0.5, 0.5))

mcGridAlive = sapply(orig.mcGrid, function(oneParamSet) {
    sum((oneParamSet[1,] == 20) | (oneParamSet[2,] > 1000)) / 
        length(oneParamSet[2,]) })

filled.contour(lambdas, kappas, 
               matrix(mcGridAlive, nrow = length(lambdas), 
                      ncol = length(kappas)), 
               xlab = "Lambda", ylab = "Kappa", 
               xlim = c(0.1, 3), ylim = c(0.1, 3.1)) 

par(oldPar)

#BA_ImageMapAtleast20Kids
oldPar  = par(mar = c(4.1, 4.1, 2, 1))

mcGridProp20kids = sapply(orig.mcGrid, function(oneParamSet) {
    sum(oneParamSet[2,] > 19) / length(oneParamSet[2,]) })

mcGridProp20kidsMat = matrix(mcGridProp20kids, 
                             nrow = length(lambdas), 
                             ncol = length(kappas))

breaks = c(0, 0.10, 0.2, 0.3, 0.5, 0.7, 0.9, 1)
colors = rev(rainbow(10))[-(1:3)]

image(lambdas, kappas, mcGridProp20kidsMat, col = colors,
      breaks = breaks, xlab = "Lambda", ylab = "Kappa", 
      xlim = c(0.05, 3.05), ylim = c(0.05, 3.05))

midBreaks = (breaks[ -8 ] + breaks[ -1 ]) / 2
par(xpd=TRUE)
legend(x = 0.1, y = 3.25, legend = midBreaks, fill = colors, bty = "n", ncol = 7)

par(oldPar)
```


## Discussion and Future Works 



## References
[1] D. Lang and D. Nolan, Data Science in R: A Case Studies Approach to Computation Reasoning and Problem Solving. New York, New York: CRC Press. 

[2] John Tsitsiklis, Christos Papadimitriou, and Pierre Humblet. The Performance of a
Precedence-Based Queuing Discipline. Journal of the Association for Computing Machinery ,
33:593–602, 1986.

[3] David Aldous and William Krebs. The ‘Birth-and-Assassination’ Process. Statistics and
Probability Letters, 10:427–430, 1990.

[4] 
