{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### title: \"Strategies for Analyzing a 12-Gigabyte Data Set: Airline Flight Delays - Case Study Unit 14\n",
    "\n",
    "output: html_notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cory Adams, Chris Boomhower, Alexandra Fisher, Alex Frye\n",
    "#### MSDS 7333, December 10, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "This case study was undertaken in order to investigate and highlight how to work with datasets that are too large in size to fit into local memory. To do this, 12 gigabytes of historical flight data for on-time performance is selected and the data is loaded and analyzed out-of-core using a branch processing method [2]. For purposes of this study, the split-apply-combine approach is used and both of the R and Python languages are leveraged to manage the data out-of-core. Results are then used to ultimately determine if consumers can reduce the number of flight delays experienced by answering key questions, such as which airports and flights are most likely to be delayed. Findings indicate the [insert airports] are the airports most likely to be delayed flying out of or into and the flights with [insert origin] as the origin and [insert destination] as the destination are the most likely to be delayed. \n",
    "\n",
    "Can we regress how delayed a flight will be before it is delayed? \n",
    "If yes, how? And what are the most important features for this regression? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this case study, the objective is not only to investigate how to work with oversized data sets too massive for local memory, but to answer key questions pertaining to airline flight performance. Specifically, a key measurement of success in the airline industry is on-time performance, which is an indicator of flight delays. The following questions of interest will be answered in this study:\n",
    "\n",
    "1.\tWhich airports are most likely to be delayed flying out of or into?\n",
    "2.\tWhich flights with same origin and destination are most likely to be delayed? \n",
    "3.\tCan we regress how delayed a flight will be before it is delayed? What are the most important features for this regression? \n",
    "\n",
    "In answering these key questions, this study will aid in determining if consumers can reduce the quantity of flight delays experienced via analyzing historical flight data collected between 1987 and 2009 [2]. The airline data set used can be found at http://stat-computing.org/dataexpo/2009/the-data.html.\n",
    "\n",
    "To best handle loading and analyzing the large airline dataset, the data must be processed out-of-core via a branch processing method. In particular, the split-apply-combine technique is utilized with various APIs. For the purpose of this study, R and Python languages are leveraged to manage the data out-of-core. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "The “Airline on-time performance” data set [2] comes from the American Statistical Association (ASA) Section on Statistical Computing and Statistical Graphics released in 2009 “for their biannual data exposition” [1]. The data set was compiled and released from the U.S. Government’s Bureau of Transportation Research and Innovative Technology Administration (RITA) and includes flight information from October 1987 to April 2008 for over 120 million flights. Flight data includes a total of 29 variables for each flight, such as flight time, delay time, departure airport, and arrival airport, equaling a total of 12 gigabytes in files. \n",
    "\n",
    "In particular, a researcher from Yale University named Michael Kane strategizes how to analyze massive datasets using the “Airline on-time performance” data set. Kane provides a case study that explains how to acquire the data set, explore the data set using different environments such as R, Unix, and SQL db and parallel computing, and model the airline data set [3 & 4]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "The steps used for this analysis were: 1) data acquisition and decompression, 2) loading data into memory using both R and Python to verify decompression, 3) preprocessing data in chunks, and 4) data analysis out-of-core using Python. \n",
    "\n",
    "Note that code used includes modified versions of R code function examples found in Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving, Chapter 5, pages 217-236 [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing in R and Python\n",
    "This is meant as a companion notebook for the the SMU MSDS program. Specifically, we will build off of chapter five in the text \"Data Science in R\", http://www.amazon.com/Data-Science-Approach-Computational-Reasoning/dp/1482234815.\n",
    "\n",
    "In this notebook we will look at ways for loading and analyzing data out-of-core using a batch processing method. Specifically, we will be using the split-apply-combine approach with a few different APIs. In R, we will use:\n",
    "- `bigmemory` for memory mapping the variables: https://cran.r-project.org/web/packages/bigmemory/index.html \n",
    "- `bigtabulate`\n",
    "- `biganalytics`\n",
    "- `doMC`\n",
    "\n",
    "In python, we will also be using a number of different libraries \n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `graphlab-create`\n",
    "\n",
    "___\n",
    "# 1.0 Downloading the dataset (using python)\n",
    "You can access descriptions of the data files from here: http://stat-computing.org/dataexpo/2009/the-data.html. You can manually download each of the zipped files OR run the following script to download the files into a folder in the same directory as this notebook called \"Data\".\n",
    "\n",
    "In the following blocks of code, we download the data and then decompress the files into .csv files. Each csv contains data for one year of airline flights. \n",
    "\n",
    "\n",
    "Note: If you want an alternative R version for performing the download operations, see:\n",
    "- http://www.cybaea.net/journal/2010/08/05/Big-data-for-R/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# create a Data directory if not done already\n",
    "path = \"Airline/\"\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir( path, 0755 )\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib # this is part of the standard library for python\n",
    "import bz2 # this is also part of the python standard library\n",
    "#from urllib.request import urlretrieve\n",
    "i = 0\n",
    "for dirpath, dirnames, files in os.walk(path):\n",
    "    if files and i==0:\n",
    "        print(\"Files already downloaded and decompressed\")\n",
    "    if not files and i==0:\n",
    "        years_to_download = range(1987,2009) # get the years 1987 through 2008\n",
    "        baseurl = 'http://stat-computing.org/dataexpo/2009/%d.csv.bz2' \n",
    "\n",
    "        files = []\n",
    "        for year in years_to_download:\n",
    "            # prepare strings\n",
    "            url_of_data_file = baseurl%(year) # get the URL for the data file\n",
    "            save_as_filename = path + '%d.csv.bz2'%(year) \n",
    "            # save as this\n",
    "            files += [save_as_filename] # save name of the compressed file\n",
    "\n",
    "            # download file\n",
    "            print('Downloading %s to %s' % (url_of_data_file, save_as_filename)) # progress update\n",
    "            urllib.urlretrieve(url_of_data_file, save_as_filename) #execute download\n",
    "            \n",
    "        # Now lets decompress all the files\n",
    "        for filename in files:#[path+'2006.csv.bz2',path+'2007.csv.bz2',path+'2008.csv.bz2']:\n",
    "            # get file names\n",
    "            filepath = filename\n",
    "            newfilepath = filename[:-4]\n",
    "            print('Decompressing', filepath,'to', newfilepath)\n",
    "\n",
    "            # go through the decompressed chunks and write out to a decompressed file\n",
    "            with open(newfilepath, 'wb') as new_file, bz2.BZ2File(filepath, 'rb') as file:\n",
    "                for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "                    new_file.write(data)\n",
    "    i = i+1\n",
    "    \n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to delete the compressed files\n",
    "Run the following block ONLY if you want to delets the compressed bz2 files from your system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!rm '/Users/darrenho/MSDSairline/'*.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 2.0 Loading data into memory\n",
    "Now that the data has downloaded and been decompressed, we can load a single file into memory to ensure that everything decompressed correctly. For each file, we could load it into memory and then save the length of the file. Let's do this in both python and in R for two files (to keep runtime at a minimum). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading individual files in python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "total_length = 0\n",
    "\n",
    "for year in [1987,1988]:\n",
    "    # get file name of the csv\n",
    "    # note that we can also load in the raw .bz2 file in python (or R) \n",
    "    # but the decompression step for these files sizes takes a huge performance hit\n",
    "    csvfile = path+'/%d.csv'%(year)\n",
    "    print('loading', csvfile)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # read the file and increment the lines count\n",
    "    %time df = pd.read_csv(csvfile) # note that this is a big operation, especially since we just want the length\n",
    "    # one way of making this shorter is to filter which columns we are loading\n",
    "    \n",
    "    total_length += len(df)\n",
    "\n",
    "print('Answer from python:', total_length)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "So now we can see that there were about 6.5M different entries in the years 1987 and 1988. However, we really want to be working with one really large descriptor of all years, rather than loading up each year from disc each time. Actually, the operations we had above are much more appropriate for a SQL query, but they are not quite as scalable for some of the other analyses that we want to do later on. \n",
    "\n",
    "The python code is noticeably faster at loading in the data, so let's use python to pre-process the csv files and make sure all the data is an integer. All the text data must be coded as numeric (an integer) when we use R's big memory package, so lets go file by file and make sure its ready to use with R.\n",
    "\n",
    "\n",
    "## 2.1 Preprocessing Data in Chunks\n",
    "In order to replace all the strings in the data frame, we first need to know exactly all the unique strings contained in each column. The first block takes a pass on all the data and finds all the unique string entries in the entire dataset. This means loading all the data files in one pass and concatenating the unique entries into a data structure. I have chosen to use a dictionary as the data structure with the name of each key in the dictionary being the column name of the variable we want to convert to an integer. The value of the key is a `set` of strings. \n",
    "\n",
    "Once we have this dataset, we can then replace the unique values properly (we will need to load the data files again!!). We can then replace the values with an integer. After replacing them, we can resave the data frame as a csv. Phew!\n",
    "\n",
    "Please note that the runtime of these blocks aggregates to ~60 minutes (or more) depending on your system. If you want an alternative implementation using R only, please see:\n",
    "- http://www.cybaea.net/journal/2010/08/05/Big-data-for-R/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this data, there are multiple variables that are objects\n",
    "# these need to be appropriately converted to numbers (integers) \n",
    "# To do this, we must know the unique values in each of the columns, let's do this first\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "# cPickle is standard on 2.x\n",
    "import cPickle as pickle\n",
    "# _pickle works on Python 3.x\n",
    "#import _pickle as pickle\n",
    "\n",
    "if os.path.isfile(path+'unique_mapping.p'):\n",
    "    print(\"Unique Mapping pickle already exists\")\n",
    "    unique_values = pickle.load(open(path+'unique_mapping.p', \"rb\"))\n",
    "    \n",
    "else:\n",
    "    unique_values = {} # create an empty dictionary of the column name an the unique values in it\n",
    "    for year in range(1987,2009):\n",
    "        t = time.time()\n",
    "        # get file name of the csv\n",
    "        csvfile = path+'%d.csv'%(year)\n",
    "        print('loading',csvfile,)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # read the file\n",
    "        df = pd.read_csv(csvfile,usecols=['Origin', 'Dest', 'UniqueCarrier','TailNum','CancellationCode']) \n",
    "        #df = df.select_dtypes(exclude=['float64','int64']) # grab only the non-numeric data\n",
    "\n",
    "        print('...finding unique values',)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        for col in df.columns:\n",
    "            print(col)\n",
    "            # check to see if we have seen this column before\n",
    "            s = set(df[col].values.astype(np.str))\n",
    "            if col not in unique_values:\n",
    "                # if not, then create a key with the unique values for that column in it\n",
    "                unique_values[col] = s\n",
    "            else:\n",
    "                # otherwise make sure that the remaining columns are unique\n",
    "                unique_values[col] |= s\n",
    "\n",
    "        print('...finished, %.2f seconds'%(time.time()-t))\n",
    "        sys.stdout.flush()\n",
    "        del df\n",
    "\n",
    "    # Save out the dictionary for later use\n",
    "    pickle.dump( unique_values, open( path+'unique_mapping.p', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's take a look at the dictionary\n",
    "print(unique_values.keys())\n",
    "print('Cancellation Code:',unique_values['CancellationCode'])\n",
    "print('UniqueCarrier:',unique_values['UniqueCarrier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install numpy --upgrade\n",
    "#!pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_numpy_replace(np_vector,replace_set):\n",
    "    # you can look at this function at your leisure, but essentially we use fast set \n",
    "    # comparison to try and speed up the analysis\n",
    "    replace_set = np.array(list(replace_set)) # get \"possible values\" as a numpy array\n",
    "    n = np.ndarray(np_vector.shape).astype(np.float64) # fill in this matrix\n",
    "    \n",
    "    vector_as_set,idx_back = np.unique(np_vector,return_inverse=True) # get the unique indices and locations\n",
    "    \n",
    "    # now loop through the unique values for this dataset\n",
    "    for idx,val in enumerate(vector_as_set):\n",
    "        # find what number this should be (like a hash)\n",
    "        category_num = np.nonzero(replace_set == val)[0][0]\n",
    "        n[idx_back==idx] = category_num # set the values as this category, vectorize for speed\n",
    "        \n",
    "    return n.astype(np.float64)\n",
    "\n",
    "if os.path.isfile(path+'AirlineDataAll.csv'):\n",
    "    print(\"AirlineDataAll.csv already exists\")\n",
    "else:\n",
    "    fileHandle = open(path+'AirlineDataAll.csv', 'w') # open and replace if needed\n",
    "    years = range(1987,2009)\n",
    "    for year in years:\n",
    "        t = time.time()\n",
    "\n",
    "        # get file name of the csv\n",
    "        csvfile = path+'%d.csv'%(year)\n",
    "        print('Running...',csvfile,)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # read the file\n",
    "        df = pd.read_csv(csvfile) \n",
    "\n",
    "        print('loaded, ...replacing values',)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # now replace the matching columnar data with the proper number category\n",
    "        for key in unique_values.keys():\n",
    "            if key in df:\n",
    "                print(key[0:4],)\n",
    "                sys.stdout.flush()\n",
    "                tmp = df[key].values.astype(np.str)\n",
    "                df[key] = fast_numpy_replace(tmp,unique_values[key])\n",
    "\n",
    "        print('...',)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        for col in df:\n",
    "            #df[col] = np.round(df[col].astype(np.float64)) # use floats to keep the nan's inline with numpy representation\n",
    "            df[col] = df[col].astype(np.float64).round(2) \n",
    "\n",
    "        print('writing',)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # these lines make one large file with the numeric data\n",
    "        # it also solves a problem with pandas closing the file that takes an inordinate amount of time\n",
    "        # NOTE: using binary here would be a huge speedup, but I am not sure about the binary structure of the \n",
    "        # backing file for bigmatrix, so we stick with CSV\n",
    "        # TODO: find out if the backing file is just a dump of the c struct to file\n",
    "        if year==years[0]:\n",
    "            df.to_csv(fileHandle,index=False, index_label=False, na_rep=\"NA\",float_format='%.0f')\n",
    "        else:\n",
    "            df.to_csv(fileHandle, mode='a', header=False, index=False, index_label=False,  na_rep=\"NA\", float_format='%.0f')\n",
    "\n",
    "        print(', %.2f sec.'%(time.time()-t))\n",
    "        del df\n",
    "\n",
    "    print('closing file',)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    fileHandle.close()\n",
    "    print('...Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now lets take a look to see what has actually changed in the file\n",
    "# let's load the head of 1987 and the big CSV file to see how they compare\n",
    "\n",
    "#this will fail for Windows, not supported\n",
    "#print('New File Format:')\n",
    "#!head 'Airline/AirlineDataAll.csv'\n",
    "#print('')\n",
    "#print('Old File Format')\n",
    "#!head 'Airline/2008.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's now look at the tail of our big dataset and the tail of the 2008 file\n",
    "# do they compare nicely?\n",
    "\n",
    "#this will fail for Windows, not supported\n",
    "#print('New File Format:')\n",
    "#!tail 'Airline/AirlineDataAll.csv'\n",
    "#print('')\n",
    "#print('Old File Format:')\n",
    "#!tail 'Airline/2008.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "In the above code, we also created a large file with all the data from every year inside of it. This is now saved as `AirlineDataAll`. Let's take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Windows does't have ls command\n",
    "#!ls -all 'Airline/' *All.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "# 4.0 Analyzing the data using only Python\n",
    "Now let's analyze the data using graphlab create (from the company Dato). As you have seen before, the graphlab-create API uses something called a scalable data frame (or SFrame) that handles all of the out-of-core memory management for you. It is by far one of the most optimized tools for handling table data out-of-core. Really. With that said, lets also give a list of what to expect:\n",
    "- Loading and saving large amounts of data will be very fast\n",
    "- Manipulating and adding columns is not much overhead, as everything is saved piece meal\n",
    "- Parallelization is mostly handled for you in the background\n",
    "- Operations are 'queued up' before execution, so that they can be simplified and parallelized (if possible and easy)\n",
    "- However, there will be slightly less flexibility in the split-apply-combine technique\n",
    " - We need to use premade aggregation function from Dato's API \n",
    " - That means we can't just write a custom function to work in parallel on the different groups (like in R)\n",
    " - But we can concatenate operations to perform rich analysis (albeit not using standard python syntax)\n",
    " - In my opinion, this is the biggest downside (that custom aggregators cannot be built), but future version of graphlab might start to support this\n",
    "- Grouping and applying cannot be separated from each other \n",
    " - This is because of the queueing of operations, grouping does not happen until absolutely neccessary \n",
    " - This also optimizes the memory management, which can be a huge time savings\n",
    "- We CAN write custom apply functions that work on each row of data (just not the grouped splits)\n",
    "- At the time of writing this, graphlab is only supported for python 2.7 (ouch!)\n",
    " - This is unfortunate because graphlab is the only reason I am not updating to python 3 ...\n",
    "- Support on windows is really good, but mac/linux is slightly better.\n",
    " - This is especially true when using Dato's numpy extensions (which only works on mac/linux)\n",
    " - And the extensions make numpy operations completely usable out-of-core, with many operations optimized for sequential access (slightly more optimized than Numpy's builtin memmap)\n",
    "\n",
    "\n",
    "## 4.1 Loading 12GB of Data in Python\n",
    "Alright, then. Let's load up graphlab in order to get an idea about the power of this tool and what the syntax looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here I needed to register (https://turi.com/register) and upgrade my graphlab using a free, academic-use-only product key:\n",
    "# pip install --upgrade --trusted-host pypi.python.org graphlab-create \n",
    "# pip install --upgrade --no-cache-dir --trusted-host get.graphlab.com https://get.graphlab.com/GraphLab-Create/2.1/dhomrighausen@smu.edu/A6C0-BEA7-AEF2-0822-AC94-D968-B66F-5B74/GraphLab-Create-License.tar.gz\n",
    "\n",
    "import graphlab as gl\n",
    "#gl.get_dependencies() #Just need to run this the first time if graphlab loads but a dependcy error ACTION REQUIRED note appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dirpath, dirnames, files in os.walk(path+'airline_data_sframe_directory'):\n",
    "    if files:\n",
    "        print(\"Binary compressed files already exists\")\n",
    "        sf = gl.load_sframe(path+'airline_data_sframe_directory')\n",
    "    if not files:\n",
    "        sf = gl.SFrame('Airline/AirlineDataAll.csv')\n",
    "        sf.save(path+'airline_data_sframe_directory') # write out as binary compressed file (very compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That was really fast to read the entire csv structure! On my system, it took about 6 minutes to prepare the SFrame. But--is it really the same length as the data we saw above? How could this be 5 times faster than the R code for loading and parsing the data? Let's check the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Yes. It is the same size! And if we dump this to file, it will be compressed to under 2GB and be written in about a minute to disk (and almost instantaneously when read--just like R). That's a really great advantage when working with large data like this. The space/time tradeoffs have really been optimized for this package. \n",
    "\n",
    "It also means that file access is quicker because the DiskIO has to load fewer bytes. It needs to decompress the bytes, but it does so typically much quicker than it would be to read them from disk. Impressive!\n",
    "\n",
    "If you wanted to save it, you could say:\n",
    "- ```sf.save('/Users/darrenho/MSDSairline/airline_data_sframe_directory') # write out as binary compressed file (very compressed)```\n",
    "- Then to load it back up:\n",
    " - ```gl.load_sframe('/Users/darrenho/MSDSairline/airline_data_sframe_directory')```\n",
    " \n",
    "___\n",
    "\n",
    "## 4.2 Preprocessing the CSV data in Python with Graphlab\n",
    "So great, we can load up the same file as R did. It still took forever to preprocess that file and get it ready. So, we really want to answer this question:\n",
    "**Is there an easier way to preprocess and concatenate all the files into one memory map using SFrames?** \n",
    "\n",
    "SFrames are more forgiving in terms of the data types that they can hold. Moreover, they load much more quickly than the parser used by `pandas`. Maybe we should try to read in each of the original CSV files as SFrames and concatenate them together into one SFrame. Would this be a quicker way to create the memory mapped file than all the preprocessing we did above?\n",
    "\n",
    "The answer is, in fact, **yes**. It is much faster and the code is much easier to understand than what we performed earlier. And because `graphlab` supports more than one data type, we do not need to preprocess any of the data. Plus, when we write out to file, it happens more quickly and in a compressed version (on disk it will be compressed to about 2GB). \n",
    "\n",
    "Here is the code to perform all the concatenation we did earlier. I am also supplying graphlab with the data type for each column to ensure that the data is consistent. However, if I did not, Graphlab would try and guess the data type based on the first 100 lines of the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''del sf # get rid of the old thing\n",
    "# What about just loading up all the data using the SFrame Utility for loading CSV files?\n",
    "# We will need to make sure that the SFrame has consistent datatypes, so we will give the value for each header\n",
    "column_hints=[int,int,int,int,int,int,int,int,str,int,str,int,int,int,int,int,str,str,int,int,int,int,str,int,int,int,int,int,int]\n",
    "\n",
    "t = time.time()\n",
    "# now load the first SFrame\n",
    "sf = gl.SFrame() #.read_csv('Data/1987.csv',column_type_hints=column_hints)\n",
    "\n",
    "# and then append each SFrame in a for loop\n",
    "for year in range(1987,2009):\n",
    "    print 'read %d lines, reading next file %d.csv'%(sf.shape[0],year)\n",
    "    sys.stdout.flush()\n",
    "    sftmp = gl.SFrame.read_csv('Airline/%d.csv'%(year),column_type_hints=column_hints)\n",
    "    sf = sf.append(sftmp)\n",
    "\n",
    "print 'It took %.2f seconds to concatenate the memory mapped file'%(time.time()-t)\n",
    "\n",
    "t = time.time()\n",
    "print 'Saving...',\n",
    "sf.save(path+'airline_data_sframe_directory') # save a compressed version of this SFrame\n",
    "print 'took %.2f seconds'%(time.time()-t),'Shape of SFrame is',sf.shape'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yeah, we concatenated and loaded the file in ~320 seconds and saved a compressed binary version in about 1 minute (on my machine). That's quite a speedup. \n",
    "\n",
    "But can we perform operations upon it? Let's check how versatile these data structures are. Let's first try to replicate the functionality of finding the most popular airports to fly out of, like we did with `bigmemory`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONVENIENCE BLOCK FOR THOSE LOADING FROM DISK HERE\n",
    "\n",
    "# If you have already run the notebook above and just want to load up the data\n",
    "# then you can reload the SFrame here\n",
    "#import graphlab as gl\n",
    "#\n",
    "#sf = gl.load_sframe('Airline/airline_data_sframe_directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Analyzing popular Airports to fly from in Python\n",
    "We now need to perform some operations using the split-apply-combine technique. However, in graphlab this all happens via one syntax (the `groupby` function). Let's see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to perform grouping and splitting\n",
    "# we need to specify (1) which column(s) to group the SFrame using, and \n",
    "#                    (2) what function we want to perform on the group\n",
    "# in graphlab, we only have a few options for performing on each of the groups. \n",
    "# Here, lets keep it simple--let's group by the airport origin and then\n",
    "#  use the builtin 'count' function to aggregate the results\n",
    "# The result is another SFrame with the Unique origin names as a column and the\n",
    "#  number of entries in each group in another column\n",
    "%time sf_counts = sf.groupby('Origin', {'num_flights':gl.aggregate.COUNT()})\n",
    "sf_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# As seen above, the sf_counts SFrame has the origin of the flight on the left\n",
    "# and the count of flights on the right \n",
    "\n",
    "# let's grab the top 10 entries\n",
    "sf_top = sf_counts.topk('num_flights',10) # this is builtin command in graphlab\n",
    "\n",
    "airports = np.array(sf_top['Origin'])\n",
    "counts = np.array(sf_top['num_flights'])\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.barh(range(len(counts)),counts)\n",
    "\n",
    "# and set them on the plot\n",
    "plt.yticks(range(len(airports)), airports)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## 4.4 Analyzing Depature Delays at Specific Times of the Day in Python\n",
    "Now, let's try to do something a little more interesting, such as trying to perform the same operations that are in the Nolan text. Specifically, let's try to find the percentiles for late flights based upon the hour of the day that they depart. To do this we will:\n",
    "- Create a new column for the hour of the day that a plane departed.\n",
    "- Fix the `hours==24` and `hours==0` to be consistent\n",
    "- Group by the hours of departure \n",
    "- Take percentiles of each group to see which ones are the latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "# first, let's create a new column in this SFrame that has the departure time floored to the nearest hour\n",
    "sf['DepTimeByHour'] = sf['CRSDepTime'].apply(lambda x: floor(x/100),dtype=int)\n",
    "sf['DepTimeByHour'] # and print a few of them (note: the column has not been evaluated yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's now change the hours of the day that are equal to 24\n",
    "# we need to be careful here becasue each column is not immutable\n",
    "# in pandas this would be:\n",
    "#    df.DepTimeByHour[df.DepTimeByHour==24] = 0\n",
    "# but we cant just change a few values in the column, we need to change them all and replace them\n",
    "# don't worry though, Graphlab does this smartly\n",
    "sf['DepTimeByHour'] = sf['DepTimeByHour'].apply(lambda x: 0 if x==24 else x)\n",
    "# again, this column has not been evaluated yet because that value has not yet been accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now lets group the SFrame by the hours and calculate the percentiles of each group\n",
    "# here is where the lazy evaluation actually happens so this takes a little while to compute\n",
    "\n",
    "# the groupby function will partition our SFrame into groups based upon the given column of data\n",
    "# next, we need to tell graphlab what operations to perform on the group and what rows\n",
    "# to do that, we send in a dictionary of names and 'operations' \n",
    "# We did a similar operation above with the 'COUNT' aggregator\n",
    "# there are only a certain number of operators we can choose from, we will choose to use the 'QUANTILE'\n",
    "#   aggregator on the column 'DepDelay'. We want to take the percentiles [0.90,0.99,0.999,0.9999]\n",
    "# We can also perform other operations by adding entries in the dictonary\n",
    "# So we will also take the 'MAX' of each group\n",
    "import time\n",
    "t = time.time()\n",
    "delay_sf = sf.groupby('DepTimeByHour', \n",
    "                            {'delay_quantiles':gl.aggregate.QUANTILE('DepDelay', [0.90,0.99,0.999,0.9999]),\n",
    "                             'delay_max':gl.aggregate.MAX('DepDelay')})\n",
    "# this returns a new SFrame with the specified columns from each aggregation\n",
    "\n",
    "print 'Took %.2f seconds to run'%(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort it by when departed and display it\n",
    "delay_sf = delay_sf.sort('DepTimeByHour')\n",
    "delay_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to use matplotlib, we need to convert over to numpy arrays\n",
    "# this is a fine operation because the new aggregated SFrame we are \n",
    "# working (delay_sf) with is quite small\n",
    "x = np.array(delay_sf['DepTimeByHour'])\n",
    "y = np.array(delay_sf['delay_quantiles'])\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x,y)\n",
    "plt.ylabel('Minutes Delayed')\n",
    "plt.xlabel('Hour of Day')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylim(0,1400) # make the same axes as in the book\n",
    "plt.legend(['0.9','0.99','0.999','0.9999'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! This doesn't look like the graph from the book at all!! Actually, there is a really great reason for that...\n",
    "\n",
    "Let's investigate further. The bottom two lines (percentiles 0.90 and 0.99) look very similar to what your book had, but the other two percentile values (0.999 and 0.9999) seemingly did not evaluate properly. \n",
    "\n",
    "Find out why by looking at the documentation for the quartile aggregator.\n",
    "- https://dato.com/products/create/docs/graphlab.data_structures.aggregation.html#module-graphlab.aggregate\n",
    "- Hint: there is a reason that the quantile calculation was really, really, really fast.\n",
    "___\n",
    "\n",
    "## 4.5 Calculate Plane Age with Dato\n",
    "So let's keep moving and try to calculate the plane's age like we did in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only use years where the tail number was recorded\n",
    "# we can manipulate the SFrame fairly easily in graphlab, so let's do it\n",
    "sf_tmp = sf[['TailNum','Year','Month','DepDelay']][sf['Year']>1994]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets try to make a function for getting the age of the plane\n",
    "# First lets just save the plane's age in years\n",
    "sf_tmp['FlightAge'] = 12*sf_tmp['Year']+sf_tmp['Month']-1\n",
    "\n",
    "# and take the minimum of that in order to get its first flight\n",
    "t = time.time()\n",
    "sf_min_ages = sf_tmp[['TailNum','FlightAge']].groupby('TailNum',{'FirstFlight':gl.aggregate.MIN('FlightAge')})\n",
    "print 'Took %.2f seconds to run'%(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now transform the FirstFlight Column into the original dataframe size\n",
    "# to do that we can just do a join on a few columns of our sf\n",
    "# this will save the flight age and the minimum in a new SFrame\n",
    "%time sf_fewcols = sf_tmp[['TailNum','FlightAge']].join(sf_min_ages,on='TailNum',how='left') # long operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and now we can simply subtract the new calculated quantity and add to the original SFrame\n",
    "sf_tmp['Age'] = sf_fewcols['FlightAge']-sf_fewcols['FirstFlight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Calculate a linear model from massive data with Python\n",
    "Graphlab (much like `biganalytics`) will perform mini-batch linear regression and it will do it fast. Let's see what kind of output we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now look at the age and delay time in terms of regression (like your book)\n",
    "%time lin_model = gl.linear_regression.create(sf_tmp['DepDelay','Age'].dropna(), target='DepDelay', features=['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_model['coefficients']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which again, gives us a positive association of Age and departure delay time (but could be very weak because we model nothing else). The value is of the same magnitude as what we found using R. It is slightly different because we need to use gradient descent (not batch methods, which are deterministic). The number of iterations and starting point for gradient methods will slightly (or not so slightly sometimes) affect the final coefficient values. \n",
    "\n",
    "___\n",
    "# 5.0 Conclusions\n",
    "Now that we have seen SFrames and bigmemory, which one should you choose? **As always, it depends.** I would recommend performing preprocessing with SFrames, almost always. They are much easier to work with. But their use of split-apply-combine is still lacking behind the abilities of R's bigmemory. Some operations are optimized, and if you can get away with only using those operations, then try just using SFrames. Since SFrames are great for adding and subtracting columns, that is another way to play around with the data before trying to use R. \n",
    "\n",
    "However, if you need a more flexible solution, bigmemory will need to be what you go with. And it is a really wonderful program... That's especially true when the aggregation function you use needs to do something rather complex. If you are running windows... SFrames are your goto. But, really, why are you running windows? Oh right, for SAS... Time to install a linux partition maybe... Or buy a mac and run bootcamp... You get the idea.\n",
    "\n",
    "Another thing to keep in mind: Graphlab is actively being developed (and most of it is open source, like SFrames). It will one day have an R interface. It will probably get custom aggregation functions one day soon. \n",
    "\n",
    "The solutions I have provided here are just examples. Large datasets can be handled using databases really well (but eventually you run into limitations). \n",
    "\n",
    "And, as always, happy analyzing!\n",
    "\n",
    "___\n",
    "\n",
    "*This Notebook was created by Professor Eric Larson and modified by Darren Homrighausen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] D. Lang and D. Nolan, Data Science in R: A Case Studies Approach to Computation Reasoning and Problem Solving. New York, New York: CRC Press.\n",
    "\n",
    "[2] H. Wickham. Airline on-time performance Web page. http://statcomputing.org/dataexpo/2009/, 2009.\n",
    "\n",
    "[3]  M. Kane and J. Emerson. biganalytics : A library of utilities for big.matrix objects of package bigmemory. http://cran.r-project.org/package=biganalytics , 2010. R package version 1.0.14.\n",
    "\n",
    "[4]  M. Kane and J. Emerson. bigmemory : Manage massive matrices with shared memory and memory-mapped files. http://cran.r-project.org/package=bigmemory , 2011. R package version 4.2.11.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Py2DM]",
   "language": "python",
   "name": "conda-env-Py2DM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
