---
title: "Using Statistics to Identify Spam - Case Study Unit 10"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

####Cory Adams, Chris Boomhower, Alexandra Fisher, Alex Frye
####MSDS 7333, November 11, 2017

***NOTE: We are Answering Q.19 from the Data Science in R textbook (Pg. 168): "Consider the other parameters that can be used to control the recursive partitioning process. Read the documentation for them in the rpart.control() documentation. Also, carry out an Internet search for more information on how to tweak the rpart() tuning parameters. Experiment with values for these parameters. Do the trees that result make sense with your understanding of how the parameters are used? Can you improve the prediction using them?"***


#Assignments (Delete This Before Submission)

* Fisher - Abstract, Intro, Background, Discussion and Future Work
* Cory - Load and Clean (disclaimer), rpart.control Research
* Chris - rpart.control Analysis and Prediction, Comparison to default
* Frye - coding...done



## Abstract


## Introduction


## Background


## Methods
The steps used for this analysis were: 1) Load and Clean the data; 2) rpart.control Parameter Research; 3) rpart.control Parameter Analysis and Prediction; 4) Comparison to Default Results.

*Note that code used includes modified versions of R code function examples found in Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving, Chapter 2, pages 22-XX , and Chapter 4, pages XXX-XXX [1].*


## Results

### Data Acquisition
```{r echo=FALSE, message=FALSE, warning = FALSE}
####Load Libraries
library(ggplot2)
library(plotly)
library(plyr)
library(grid)
library(gridExtra)
library(formattable)
library(spam)
library(RColorBrewer)
library(rpart)
library(dplyr)
library(corrplot)
library(PerformanceAnalytics)
library(rpart.plot)
library(rpart)
library(RColorBrewer)
library(tm)
```

```{r include=FALSE, cache=TRUE, echo = FALSE, warning=FALSE}
source('DataExtractAndClean.R', echo = FALSE)
```


### rpart.control Parameter Research

Will adjust following parameters:

  * cp (control)
  * minsplit (control) 2:1 with minbucket
  * minbucket (control) 1:2 with minsplit
  * maxdepth (control)

Below code copied from DataExtractAndClean.R but needs modified to work with above parameters. 


Total parameter combinations generated: ````r paramCombos```` <br>
Parameter Combinations Sampled to: ````r paramCombos/5````

```{r}
#cost<-data.frame(cost=c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 5.0))

cp<-data.frame(cp=c(0.00001, 0.0001, 0.005, 0.01))

bucketSplit<-data.frame(minsplit =round(seq(2, 100, length=30),0)+ (round(seq(2, 100, length=30),0)%%2))
bucketSplit$minbucket <- bucketSplit$minsplit/2

maxDepth<-data.frame(maxDepth = round(seq(3, 30, length=10),0))

#parameters<-merge(merge(merge(cost,cp,all=TRUE),bucketSplit,all=TRUE),maxDepth,all=TRUE)
parameters<-merge(merge(cp,bucketSplit,all=TRUE),maxDepth,all=TRUE)

paramCombos <- nrow(parameters)

set.seed(paramCombos)

paramSampleddf<- sample_n(parameters, (paramCombos/5))

formattable(head(paramSampleddf,10))

```

### rpart.control Parameter Analysis and Prediction
Having now defined our 240 unique combinations of cp, minsplit, minbucket, and maxDepth, we proceed with rendering our various partitioning trees to compare performance. Below, we segment each configuration into a list of 240 unique parameter combinations and then use lapply() to apply the rpart() function using each configuration. 
```{r}
paramSampled<-setNames(split(paramSampleddf, seq(nrow(paramSampleddf))), rownames(paramSampleddf))

fits <- lapply(paramSampled, function(x) {
  rpartObj = rpart(isSpam ~ ., data = trainDF,
                   method="class", 
                   #cost = x$cost,
                   control = rpart.control(cp = x$cp, minsplit = x$minsplit, minbucket = x$minbucket, maxdepth = x$maxDepth) )
  
  predict(rpartObj, 
          newdata = testDF[ , names(testDF) != "isSpam"],
          type = "class")
  })

```

Upon implementing the above code chunk, a new list of prediction results (length 240) for the 3116 test messages is rendered. This means we now have 240 sets of prediction results for all 3116 test messages. We use this list to compute accuracies and compare overall classification performance of each input parameter configuration set.

Prediction accuracies for each parameter combination are calculated next. This is done by processing our list of results and creating a new accuracy column in our previous parameter combination dataframe. The top 10 most accurate combinations are presented in the table that follows.

```{r warning=FALSE}
spam = testDF$isSpam == "T"

accuracy = lapply(fits,function(x){
  xtab <- table(unlist(x), spam)
  #print(xtab)
  sum(diag(xtab))/sum(xtab)

  })

paramSampleddf$Accuracies <- unlist(accuracy)

maxAccuracy <- max(paramSampleddf$Accuracies)
formattable(head(paramSampleddf[order(-paramSampleddf$Accuracies),],10))
```

While this table is informative, the fact that we sampled the 1200 possible parameter combinations down to 240 makes it difficult to interpret with much granularity. For this reason, we take a novel approach to parameter assessment next. We've rendered the following scatterplot and correlation matrix comprised of rpart()/rpart.control() parameters and final accuracies to better assess performance. By visually treating accuracy as we would a response variable, and cp, minsplit, minbucket, and maxDepth as we would independent variables, we render the following scatterplot and correlation matrix.

```{r warning=FALSE}
res <- cor(paramSampleddf)

chart.Correlation(paramSampleddf, histogram=TRUE, pch=19)

```

Each dot represents.....will continue Tuesday

Notes for interpretation:
In the above plot:

* The distribution of each variable is shown on the diagonal.
* On the bottom of the diagonal : the bivariate scatter plots with a fitted line are displayed
* On the top of the diagonal : the value of the correlation plus the significance level as stars
* Each significance level is associated to a symbol : p-values(0, 0.001, 0.01, 0.05, 0.1, 1) <=> symbols("***", "**", "*", ".", " ")


### Comparison to Default Results

````r (maxAccuracy-defaultAccuracy)*100````% increase in accuracy

```{r}
#code here to run defaults

rpartObj = rpart(isSpam ~ ., data = trainDF, method="class")

defaultResults <- predict(rpartObj, 
        newdata = testDF[ , names(testDF) != "isSpam"],
        type = "class")

xtab <- table(unlist(defaultResults), spam)
#print(xtab)
defaultAccuracy<-sum(diag(xtab))/sum(xtab)
#print(defaultAccuracy)
```


## Discussion and Future Works 


## References
[1] D. Lang and D. Nolan, Data Science in R: A Case Studies Approach to Computation Reasoning and Problem Solving. New York, New York: CRC Press. 
