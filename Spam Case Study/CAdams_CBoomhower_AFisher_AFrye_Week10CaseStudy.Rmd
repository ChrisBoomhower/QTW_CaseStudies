---
title: "Using Statistics to Identify Spam - Case Study Unit 10"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

####Cory Adams, Chris Boomhower, Alexandra Fisher, Alex Frye
####MSDS 7333, November 11, 2017

***NOTE: We are Answering Q.19 from the Data Science in R textbook (Pg. 168): "Consider the other parameters that can be used to control the recursive partitioning process. Read the documentation for them in the rpart.control() documentation. Also, carry out an Internet search for more information on how to tweak the rpart() tuning parameters. Experiment with values for these parameters. Do the trees that result make sense with your understanding of how the parameters are used? Can you improve the prediction using them?"***

#Assignments (Delete This Before Submission)
* Fisher - Abstract, Intro, Background, Discussion and Future Work
* Cory - Load and Clean (disclaimer), rpart.control Research
* Chris - rpart.control Analysis and Prediction, Comparison to default
* Frye - coding...done

## Abstract
In this case study, over 9,000 email messages are examined in order to develop and test spam filters via classification trees and recursive partitioning. Spam filters are ubiquitous today among email providers and are designed to examine content for red flags as indicators of a wanted or unwanted message, respectively known as ham or spam. Spam data used comes from messages previously classified as either ham or spam by SpamAssassin (http://spamassassin.apache.org) specifically for creating and testing these spam filters. R statistical language is leveraged for spam data cleanup and text processing through the use of decision trees. The decision tree approach uses message characteristics to derive sets of features for email classification, and to build a decision tree from derived features, a recursive partitioning method is applied.

## Introduction
This case study was undertaken in order to explore and shed light on automating classification procedures for filtering unwanted and potentially harmful email messages, referred to hereafter as spam. Spam filters have become ubiquitous among all email providers and are designed to closely examine content for red flags. After analyzing content for red flag characteristics, spam filters then determine if the message is either a wanted/desired message (ham) or an unwanted message (spam). These types of determinations are rooted in statistical analyses of many emails previously classified as either ham or spam. 

Spam data used comes from 9,000 messages previously classified by SpamAssassin (http://spamassassin.apache.org) specifically for creating and testing these spam filters. First and foremost, the information contained in the messages must be organized and processed in order to correctly quantify for further analysis. R statistical language is leveraged to conduct text processing for this initial step and two approaches can be utilized—text mining or decision trees. The text mining approach simply tallies words and compares associated frequencies in ham versus spam, while the decision tree approach uses message characteristics to derive variables for email classification. For the purposes of this case study, only the decision tree approach is used. 

To read the 9,000 email messages into R, each file is read as its own message. Various parts are then identified within each message as one of the following: (1) the header (sender and subject information); (2) the message body; or (3) an attachment. However, prior to designing data extraction, we investigate email structures to detail and understand the general message format. Then the headers and message bodies can be processed for extraction. Examples of key information gathered include identifying excessive amounts of punctuation and capitalization in the subject line or character count and frequency in the body. The decision tree is then used to derive variables associated with certain characteristics to classify the messages. 

## Background
To build a decision tree from derived features, a recursive partitioning method is applied. 


## Methods
The steps used for this analysis were: 1) Load and Clean the data; 2) rpart.control Parameter Research; 3) rpart.control Parameter Analysis and Prediction; 4) Comparison to Default Results.

*Note that code used includes modified versions of R code function examples found in Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving, Chapter 3, pages 107-164 [1].*

## Results

### Data Acquisition
```{r echo=FALSE, message=FALSE, warning = FALSE}
####Load Libraries
library(ggplot2)
library(plotly)
library(plyr)
library(grid)
library(gridExtra)
library(formattable)
library(spam)
library(RColorBrewer)
library(rpart)
library(dplyr)
library(corrplot)
library(PerformanceAnalytics)
library(rpart.plot)
library(rpart)
library(RColorBrewer)
library(tm)
```

To begin, we utilized methods provided in the Data Science in R textbook to load a "clean" dataset for spam and ham samples and the associated features. As the purpose of this analysis is to research this data post load, we have excluded the text processing and feature engineering from this analysis. The data is loaded external to the notebook in the ```DataExtractAndClean.R``` file allowing us to focus on our analysis of parameter tuning.

```{r include=FALSE, cache=TRUE, echo = FALSE, warning=FALSE}
source('DataExtractAndClean.R', echo = FALSE)
```


### rpart.control Parameter Research

We have spent significant time to process the data and output features for our model development. However, the goal is fit the best model possible with this data. Once you feel your data is "clean" and accurate, performance gains usually come in the form of tuning the parameters passed to the algorithm. This project uses rpart partitioning algorithm, which exposes several parameters/arguments through the rpart.control function. This function enables us to control each parameter input and compare the results. However, the rpart.control function does not accept a list of values for each parameter to generate all possible combinations of the argument values. This is a limitation we must address with custom code.

The code below will create 1200 combinations of these parameter values we can then pass to the control function for evaluation and identification of the optimal paramater value combination. However, 1200 paramater combinations requires significant processing time. For this reason we sampled the 1200 combinations and selected 240 for processing.

This experiment will focus on the following parameters as input to rpart.control:

  * cp (control) - This parameter is to optimize computing time by removing splits that do not increase the R-squared value by the defined cp value at each step. For this project we have chosen the following cp values: 0.00001, 0.0001, 0.005, and 0.01.
  
  * minsplit (control) - rpart builds models that result in tree. This tree must have branches and values leading to an end node result. The minsplit parameter defines the number of observations necessary for a split or branch to be considered. For this project we have chosen to have a 2:1 ratio with minbucket. Keeping the same ratio allows us to normalize the parameter values.

  * minbucket (control) - This parameter determines the lowest possible value for the number of observations in an end node. For this project we have chosen to have a 1:2 ratio with minsplit. Keeping the same ratio allows us to normalize the parameter values.
  
  * maxdepth (control) - This parameter defines how many levels of nodes (splits) the final output tree may contain. If the tree contains too many levels, or too much depth, it will likely output poor results (potential overfitting). 
  
Again, the code below will output a dataframe with 240 parameter combinations sampled from the initial 1200 parameter combinations created using the cp, minsplit, minbucket, and maxdepth parameters. Each of these parameter combinations are now ready to be passed to the rpart.control function (in a loop) to identify the optimal combination. The graphic below shows the first 10 combinations of sampled parameters merged into a single dataframe. This provides a good representation of the parameter combinations we developed and demonstrates the spread of values tested.

Total parameter combinations generated: ````r paramCombos```` <br>
Parameter Combinations Sampled to: ````r paramCombos/5````

```{r}
#cost<-data.frame(cost=c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 5.0))

cp<-data.frame(cp=c(0.00001, 0.0001, 0.005, 0.01))

bucketSplit<-data.frame(minsplit =round(seq(2, 100, length=30),0)+ (round(seq(2, 100, length=30),0)%%2))
bucketSplit$minbucket <- bucketSplit$minsplit/2

maxDepth<-data.frame(maxDepth = round(seq(3, 30, length=10),0))

#parameters<-merge(merge(merge(cost,cp,all=TRUE),bucketSplit,all=TRUE),maxDepth,all=TRUE)
parameters<-merge(merge(cp,bucketSplit,all=TRUE),maxDepth,all=TRUE)

paramCombos <- nrow(parameters)

set.seed(paramCombos)

paramSampleddf<- sample_n(parameters, (paramCombos/5))

formattable(head(paramSampleddf,10))

```


### rpart.control Parameter Analysis and Prediction
Having now defined our 240 unique combinations of cp, minsplit, minbucket, and maxDepth, we proceed with rendering our various partitioning trees to compare performance. Below, we segment each configuration into a list of 240 unique parameter combinations and then use lapply() to apply the rpart() function using each configuration. 
```{r}
paramSampled<-setNames(split(paramSampleddf, seq(nrow(paramSampleddf))), rownames(paramSampleddf))

fits <- lapply(paramSampled, function(x) {
  rpartObj = rpart(isSpam ~ ., data = trainDF,
                   method="class", 
                   #cost = x$cost,
                   control = rpart.control(cp = x$cp, minsplit = x$minsplit, minbucket = x$minbucket, maxdepth = x$maxDepth) )
  
  predict(rpartObj, 
          newdata = testDF[ , names(testDF) != "isSpam"],
          type = "class")
  })

```

Upon implementing the above code chunk, a new list of prediction results (length 240) for the 3116 test messages is rendered. This means we now have 240 sets of prediction results for all 3116 test messages. We use this list to compute accuracies and compare overall classification performance of our trees using each input parameter configuration set.

Prediction accuracies using each parameter combination are calculated next. This is done by processing our list of results and creating a new accuracy column in our previous parameter combination dataframe. The top 10 most accurate combinations are presented in the table that follows.

```{r warning=FALSE}
spam = testDF$isSpam == "T"

accuracy = lapply(fits,function(x){
  xtab <- table(unlist(x), spam)
  #print(xtab)
  sum(diag(xtab))/sum(xtab)

  })

paramSampleddf$Accuracies <- unlist(accuracy)

maxAccuracy <- max(paramSampleddf$Accuracies)
formattable(head(paramSampleddf[order(-paramSampleddf$Accuracies),],10))
```

While this table is informative, the fact that we sampled our 1200 parameter combinations down to 240 makes it difficult to interpret with much granularity. For this reason, we take a novel approach to parameter assessment next. We've rendered the following scatterplot and correlation matrix comprised of rpart()/rpart.control() parameters and final accuracies to better assess performance. By visually treating accuracy as we would a response variable, and cp, minsplit, minbucket, and maxDepth as we would independent variables, we are able to observe correlations between each parameter and model accuracy.

```{r warning=FALSE}
res <- cor(paramSampleddf)

chart.Correlation(paramSampleddf, histogram=TRUE, pch=19)

```

In our above scatterplot and correlation matrix, the distribution of each rpart() or rpart.control() parameter is shown along the diagonal. Reviewing these histogram distributions reveals that a mostly even distribution of parameter values made it into the 240 configuration samples. This suggests we are assessing a wide range of model accuracy outcomes based on parameter selection.

Further supporting this argument, the bottom-left scatterplots depict bivariate relationships among the parameter values. A rolling fit line is also provided in these plots, but like the general bivariate relationships among parameters, we only care about the fit line trend for the bottom row where parameters are compared against accuracy.

In the top-right cells, bivariate correlations are provided (Pearson's R values). Similarly, relationship significance levels are indicated by stars. Three stars represent a p-value less than 0.001; two stars, a p-value between 0.001 and 0.01; one star, a p-value between 0.01 and 0.05; and a period, a p-value between 0.05 and 0.1. Again, these values are only important when comparing parameters against accuracy. So in this case, only the values in the rightmost column are relevant to us.

Based on these matrix descriptions, we can quickly determine that accuracy decreases slightly as cp increases (weak negative correlation of -0.17), accuracy decreases more rapidly as either minsplit or minbucket increases (moderate negative correlation of -0.4 for each), and accuracy improves as maxDepth is increased (moderate positive correlation of 0.51). Reviewing the scatterplots and fit lines, we discern that while cp, minsplit, and minbucket relationships to accuracy are relatively linear, maxDepth is not. In contrast, it portrays a change in trend around a maxDepth value of 14. Prior to this value, accuracy rapidly improves as maxDepth increases, but after this value, improvement to accuracy is virtually non-existant. Therefore, a maxDepth of 14 seems to be the point after which we see diminishing returns when increasing our maxDepth value.

Based on these evaluations, it is clear why a cp value of 0.00001, minsplit of 2, minbucket of 1, and maxDepth of 21 produced the most accurate model. The smallest cp, minsplit, and minbucket values, and larger maxDepth values produce the best results. Granted, our accuracy would have been relatively the same had we had a maxDepth value of 14; again, we only sampled 240 parameter configurations among our original 1200. Based on our discussions in the *rpart.control Parameter Research* section, the new trees generated using these input parameters make sense.

### Comparison to Default Results

Finally, with our best tree parameter combination identified above, we may compare performance against this case study's original tree's accuracy (````r defaultAccuracy*100````%). The original tree was configured with default parameters as depicted in the following code chunk. By implementing our tuned partitioning tree, we are able to gain an accuracy improvement of ````r (maxAccuracy-defaultAccuracy)*100````%. Given the minimal effort required to tune our model with these parameters alone, this is a satisfactory improvement indeed.

```{r}
#code here to run defaults

rpartObj = rpart(isSpam ~ ., data = trainDF, method="class")

defaultResults <- predict(rpartObj, 
        newdata = testDF[ , names(testDF) != "isSpam"],
        type = "class")

xtab <- table(unlist(defaultResults), spam)
defaultAccuracy<-sum(diag(xtab))/sum(xtab)
```


## Discussion and Future Works 
* Discuss cross validation and the utilization of the "xval" parameter in rpart.control
* Also, to take a more manual approach to modifications to inputs surrounding our winner, due to the "random" nature of our parameter tests, there may be further winnings with additional research
* Finally, precision and recall as a supplement to our result metric to ensure we have chosen the best model
* As an extension of this effort, future works would include a deeper investigation into relationships among derived variables and employing a text mining Naïve Bayes approach for an side-by-side comparison of model predicitons. 

## References
[1] D. Lang and D. Nolan, Data Science in R: A Case Studies Approach to Computation Reasoning and Problem Solving. New York, New York: CRC Press. 
[2] B. Atkinson, B. Ripley, & T. Therneau, Package 'rpart'. Version 4.1-11. Online: 2017-03-12. https://cran.r-project.org/web/packages/rpart/rpart.pdf 
